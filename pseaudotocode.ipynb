{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad712966",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-10T08:48:53.933711Z",
     "iopub.status.busy": "2025-03-10T08:48:53.933477Z",
     "iopub.status.idle": "2025-03-10T08:49:00.064673Z",
     "shell.execute_reply": "2025-03-10T08:49:00.063965Z"
    },
    "papermill": {
     "duration": 6.136168,
     "end_time": "2025-03-10T08:49:00.066351",
     "exception": false,
     "start_time": "2025-03-10T08:48:53.930183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import torch\n",
    "# Download the 'punkt' tokenizer data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"/kaggle/input/spoc-db/spoc-train-train.tsv\"  # Change to your dataset's path\n",
    "df = pd.read_csv(file_path, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665249a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T08:49:00.072023Z",
     "iopub.status.busy": "2025-03-10T08:49:00.071761Z",
     "iopub.status.idle": "2025-03-10T08:50:36.834121Z",
     "shell.execute_reply": "2025-03-10T08:50:36.832980Z"
    },
    "papermill": {
     "duration": 96.769333,
     "end_time": "2025-03-10T08:50:36.838357",
     "exception": false,
     "start_time": "2025-03-10T08:49:00.069024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data saved to tokenized_spoc.csv\n"
     ]
    }
   ],
   "source": [
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")  # Replace NaN with empty strings\n",
    "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)\n",
    "df[\"text_tokens\"] = df[\"text\"].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
    "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)\n",
    "df[\"code_tokens\"] = df[\"code\"].apply(word_tokenize)\n",
    "# Add start and end tokens to tokenized C++ code\n",
    "df[\"code_tokens\"] = df[\"code_tokens\"].apply(lambda tokens: [\"<start>\"] + tokens + [\"<end>\"])\n",
    "max_len = max(df[\"text_tokens\"].apply(len).max(), df[\"code_tokens\"].apply(len).max())\n",
    "df[\"text_tokens\"] = df[\"text_tokens\"].apply(lambda tokens: tokens + [\"<pad>\"] * (max_len - len(tokens)))\n",
    "df[\"code_tokens\"] = df[\"code_tokens\"].apply(lambda tokens: tokens + [\"<pad>\"] * (max_len - len(tokens)))\n",
    "output_file = \"tokenized_spoc.csv\"\n",
    "df[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Tokenized data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c059da4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T08:50:36.843845Z",
     "iopub.status.busy": "2025-03-10T08:50:36.843602Z",
     "iopub.status.idle": "2025-03-10T08:50:40.529239Z",
     "shell.execute_reply": "2025-03-10T08:50:40.528232Z"
    },
    "papermill": {
     "duration": 3.689874,
     "end_time": "2025-03-10T08:50:40.530678",
     "exception": false,
     "start_time": "2025-03-10T08:50:36.840804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to tokenizer_vocab.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define special tokens with fixed indices\n",
    "vocab = {\n",
    "    \"<unk>\": 0,\n",
    "    \"<pad>\": 1,\n",
    "    \"<start>\": 2,\n",
    "    \"<end>\": 3\n",
    "}\n",
    "\n",
    "# Assign indices to other tokens\n",
    "for column in [\"text_tokens\", \"code_tokens\"]:\n",
    "    for tokens in df[column]:\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "\n",
    "# Save vocabulary to JSON\n",
    "vocab_file = \"tokenizer_vocab.json\"\n",
    "with open(vocab_file, \"w\") as f:\n",
    "    json.dump(vocab, f, indent=4)\n",
    "\n",
    "print(f\"Vocabulary saved to {vocab_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4519e5c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T08:50:40.536505Z",
     "iopub.status.busy": "2025-03-10T08:50:40.536240Z",
     "iopub.status.idle": "2025-03-10T08:51:44.627096Z",
     "shell.execute_reply": "2025-03-10T08:51:44.626117Z"
    },
    "papermill": {
     "duration": 64.097359,
     "end_time": "2025-03-10T08:51:44.630547",
     "exception": false,
     "start_time": "2025-03-10T08:50:40.533188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequences saved to sequences.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"tokenizer_vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Load tokenized data\n",
    "df = pd.read_csv(\"/kaggle/working/tokenized_spoc.csv\")\n",
    "\n",
    "# Convert string tokens to lists\n",
    "df[\"text_tokens\"] = df[\"text_tokens\"].apply(eval)\n",
    "df[\"code_tokens\"] = df[\"code_tokens\"].apply(eval)\n",
    "\n",
    "# Convert tokens to sequences using vocabulary\n",
    "df[\"text_sequences\"] = df[\"text_tokens\"].apply(lambda tokens: [vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\n",
    "df[\"code_sequences\"] = df[\"code_tokens\"].apply(lambda tokens: [vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\n",
    "\n",
    "# Save sequences to CSV\n",
    "output_file = \"sequences.csv\"\n",
    "df[[\"text_sequences\", \"code_sequences\"]].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Tokenized sequences saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2443b05d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T08:51:44.636537Z",
     "iopub.status.busy": "2025-03-10T08:51:44.636275Z",
     "iopub.status.idle": "2025-03-10T08:52:43.938832Z",
     "shell.execute_reply": "2025-03-10T08:52:43.937992Z"
    },
    "papermill": {
     "duration": 59.307178,
     "end_time": "2025-03-10T08:52:43.940274",
     "exception": false,
     "start_time": "2025-03-10T08:51:44.633096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Batches:   0%|          | 0/3846 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DataLoad(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        self.inputs = [ast.literal_eval(x) for x in df['text_sequences']]\n",
    "        self.outputs = [ast.literal_eval(x) for x in df['code_sequences']]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n",
    "        output_tensor = torch.tensor(self.outputs[idx], dtype=torch.int64)\n",
    "        return input_tensor, output_tensor\n",
    "\n",
    "def Add_Pad(batch):\n",
    "    inputs, outputs = zip(*batch)\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    outputs = pad_sequence(outputs, batch_first=True, padding_value=0)\n",
    "    return inputs, outputs\n",
    "\n",
    "# Load dataset and dataloader\n",
    "dataset = DataLoad('/kaggle/working/sequences.csv')\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=Add_Pad)\n",
    "\n",
    "# Iterate with progress bar\n",
    "data_iter = iter(dataloader)\n",
    "for batch in tqdm(dataloader, desc=\"Loading Batches\"):\n",
    "    features, labels = batch  # Get a batch of data\n",
    "    break  # Remove this if you want to iterate over all batches\n",
    "\n",
    "print(\"Batch loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7725c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T08:52:43.946526Z",
     "iopub.status.busy": "2025-03-10T08:52:43.946278Z",
     "iopub.status.idle": "2025-03-10T08:52:44.363436Z",
     "shell.execute_reply": "2025-03-10T08:52:44.362498Z"
    },
    "papermill": {
     "duration": 0.421766,
     "end_time": "2025-03-10T08:52:44.364787",
     "exception": false,
     "start_time": "2025-03-10T08:52:43.943021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# Transformer Hyperparameters\n",
    "class Config:\n",
    "    vocab_size = 12388  # Adjust based on vocabulary.json\n",
    "    max_length = 100  # Adjust based on sequence length\n",
    "    embed_dim = 256\n",
    "    num_heads = 8\n",
    "    num_layers =2\n",
    "    feedforward_dim = 512\n",
    "    dropout = 0.1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # Shape: (1, max_len, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Transformer Model\n",
    "class PseudoCodeTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PseudoCodeTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(config.embed_dim, config.max_length)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=config.embed_dim,\n",
    "            nhead=config.num_heads,\n",
    "            num_encoder_layers=config.num_layers,\n",
    "            num_decoder_layers=config.num_layers,\n",
    "            dim_feedforward=config.feedforward_dim,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(config.embed_dim, config.vocab_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(config.device)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding(src) * math.sqrt(config.embed_dim)\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(config.embed_dim)\n",
    "\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "\n",
    "        src_mask = self.generate_square_subsequent_mask(src.size(1))\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
    "\n",
    "        out = self.transformer(src_emb.permute(1, 0, 2), tgt_emb.permute(1, 0, 2),\n",
    "                               src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        out = self.fc_out(out.permute(1, 0, 2))  # Convert back to batch-first\n",
    "        return out\n",
    "\n",
    "# Initialize Model\n",
    "model = PseudoCodeTransformer(config).to(config.device)\n",
    "print(\"Model initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729bda9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T08:52:44.371535Z",
     "iopub.status.busy": "2025-03-10T08:52:44.371275Z",
     "iopub.status.idle": "2025-03-10T08:52:44.376911Z",
     "shell.execute_reply": "2025-03-10T08:52:44.376277Z"
    },
    "papermill": {
     "duration": 0.010282,
     "end_time": "2025-03-10T08:52:44.378035",
     "exception": false,
     "start_time": "2025-03-10T08:52:44.367753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate(model, pseudocode_tokens, vocab, device, max_length=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert pseudocode tokens to numerical indices\n",
    "    input_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in pseudocode_tokens]\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Start token for generation\n",
    "    output_ids = [vocab[\"<start>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output_tensor = torch.tensor(output_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor, output_tensor)\n",
    "\n",
    "        # Select the most probable token\n",
    "        next_token_id = predictions.argmax(dim=-1)[:, -1].item()\n",
    "        output_ids.append(next_token_id)\n",
    "\n",
    "        # Stop if end token is generated\n",
    "        if next_token_id == vocab[\"<end>\"]:\n",
    "            break\n",
    "\n",
    "    # Convert token indices back to words\n",
    "    id_to_token = {idx: token for token, idx in vocab.items()}\n",
    "    translated_code = [id_to_token.get(idx, \"<unk>\") for idx in output_ids[1:]]  # Exclude <start> token\n",
    "\n",
    "    return \" \".join(translated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14282b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T08:52:44.384432Z",
     "iopub.status.busy": "2025-03-10T08:52:44.384139Z",
     "iopub.status.idle": "2025-03-10T09:13:43.505593Z",
     "shell.execute_reply": "2025-03-10T09:13:43.504576Z"
    },
    "papermill": {
     "duration": 1259.126347,
     "end_time": "2025-03-10T09:13:43.507106",
     "exception": false,
     "start_time": "2025-03-10T08:52:44.380759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 3846/3846 [04:12<00:00, 15.26it/s, loss=0.071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.9334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 3846/3846 [04:11<00:00, 15.27it/s, loss=0.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 3846/3846 [04:11<00:00, 15.31it/s, loss=0.202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.3663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 3846/3846 [04:10<00:00, 15.38it/s, loss=0.817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.3151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 3846/3846 [04:10<00:00, 15.36it/s, loss=0.691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.2820\n",
      "Model saved: checkpoints/PseudoToCode_transformer.pth\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load vocabulary\n",
    "with open(\"tokenizer_vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Loss Function & Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)  # Ignore padding token\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
    "\n",
    "# Create directory to save models\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)  # Move batch to GPU\n",
    "\n",
    "        tgt_input = tgt[:, :-1]  # Remove <end> token\n",
    "        tgt_output = tgt[:, 1:]  # Shifted version\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "\n",
    "        loss = criterion(output.view(-1, config.vocab_size), tgt_output.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader) \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the entire model after all epochs\n",
    "final_model_path = \"checkpoints/PseudoToCode_transformer.pth\"\n",
    "torch.save(model, final_model_path)\n",
    "print(f\"Model saved: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796d31b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T09:13:46.320970Z",
     "iopub.status.busy": "2025-03-10T09:13:46.320488Z",
     "iopub.status.idle": "2025-03-10T09:13:46.479594Z",
     "shell.execute_reply": "2025-03-10T09:13:46.478689Z"
    },
    "papermill": {
     "duration": 1.551435,
     "end_time": "2025-03-10T09:13:46.480935",
     "exception": false,
     "start_time": "2025-03-10T09:13:44.929500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Example Prediction (Pseudocode to Code): if ( x > 10 10 10 10 10 10 10 ) cout < < < x > 10 < x < x < endl ; <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_pseudocode = \"if x is greater than 10 then print x\"\n",
    "tokenized_input = example_pseudocode.split()\n",
    "translated_code = translate(model, tokenized_input, vocab, device)\n",
    "print(f\"Final Example Prediction (Pseudocode to Code): {translated_code}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6797893,
     "sourceId": 10932421,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1499.323163,
   "end_time": "2025-03-10T09:13:50.718640",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-10T08:48:51.395477",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
